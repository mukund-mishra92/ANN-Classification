{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Usage Frequency</th>\n",
       "      <th>Support Calls</th>\n",
       "      <th>Payment Delay</th>\n",
       "      <th>Subscription Type</th>\n",
       "      <th>Contract Length</th>\n",
       "      <th>Total Spend</th>\n",
       "      <th>Last Interaction</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Annual</td>\n",
       "      <td>932.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>557.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Quarterly</td>\n",
       "      <td>185.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Standard</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>396.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>617.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  Tenure  Usage Frequency  Support Calls  Payment Delay  \\\n",
       "0  30.0       0    39.0             14.0            5.0           18.0   \n",
       "1  65.0       0    49.0              1.0           10.0            8.0   \n",
       "2  55.0       0    14.0              4.0            6.0           18.0   \n",
       "3  58.0       1    38.0             21.0            7.0            7.0   \n",
       "4  23.0       1    32.0             20.0            5.0            8.0   \n",
       "\n",
       "  Subscription Type Contract Length  Total Spend  Last Interaction  Churn  \n",
       "0          Standard          Annual        932.0              17.0    1.0  \n",
       "1             Basic         Monthly        557.0               6.0    1.0  \n",
       "2             Basic       Quarterly        185.0               3.0    1.0  \n",
       "3          Standard         Monthly        396.0              29.0    1.0  \n",
       "4             Basic         Monthly        617.0              20.0    1.0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/balmukundmishra/Desktop/2025-Learning/ANN_Classification/data/customer_churn_dataset-training-master.csv')  \n",
    "# data.head()\n",
    "\n",
    "L_encode = LabelEncoder()\n",
    "st_scaler = StandardScaler()\n",
    "one_hot_encode_contr = OneHotEncoder()\n",
    "one_hot_encode_subs = OneHotEncoder()\n",
    "\n",
    "data.drop(columns=['CustomerID'], inplace=True)\n",
    "\n",
    "data ['Gender'] = L_encode.fit_transform(data['Gender'])\n",
    "# # #data ['Subscription Type'] = L_encode.fit_transform(data['Subscription Type'])\n",
    "one_hot_contract = one_hot_encode_contr.fit_transform(data[['Contract Length']])\n",
    "\n",
    "\n",
    "one_hot_subsc = one_hot_encode_subs.fit_transform(data[['Subscription Type']])\n",
    "\n",
    "# data['Age'] = st_scaler.fit_transform(data[['Age']])\n",
    "# data['Tenure'] = st_scaler.fit_transform(data[['Tenure']])\n",
    "# data['Usage Frequency'] = st_scaler.fit_transform(data[['Usage Frequency']])\n",
    "# data['Support Calls'] = st_scaler.fit_transform(data[['Support Calls']])\n",
    "# data['Payment Delay'] = st_scaler.fit_transform(data[['Payment Delay']])\n",
    "# data['Total Spend'] = st_scaler.fit_transform(data[['Total Spend']])\n",
    "# data['Last Interaction'] = st_scaler.fit_transform(data[['Last Interaction']])\n",
    "\n",
    "data.head()\n",
    "\n",
    "#data.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_contract.toarray()\n",
    "\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_contract.toarray(),columns=one_hot_encode_contr.get_feature_names_out(['Contract Length']))\n",
    "one_hot_encoded_df1 = pd.DataFrame(one_hot_subsc.toarray(), columns=one_hot_encode_subs.get_feature_names_out(['Subscription Type']))\n",
    "\n",
    "data = pd.concat([data, one_hot_encoded_df, one_hot_encoded_df1], axis=1)\n",
    "data.drop(columns=['Contract Length', 'Subscription Type'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the encoder and scaler\n",
    "\n",
    "# = OneHotEncoder()\n",
    "with open('one_hot_encode_contr.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encode_contr, f)\n",
    "\n",
    "with open('one_hot_encode_subs.pkl', 'wb') as f:\n",
    "    pickle.dump(one_hot_encode_subs, f)\n",
    "\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(L_encode, f)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "    \n",
    "X = data.drop(columns=['Churn'])\n",
    "y = data['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=42, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = st_scaler.fit_transform(X_train)\n",
    "X_test = st_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67189048, -1.14620805,  1.66573772, ..., -0.71357095,\n",
       "         1.39790787, -0.00168391],\n",
       "       [-1.47583357, -1.14620805, -0.65258846, ..., -0.71357095,\n",
       "         1.39790787, -0.00168391],\n",
       "       [ 0.61441846, -1.14620805, -1.23217   , ..., -0.71357095,\n",
       "        -0.71535472, -0.00168391],\n",
       "       ...,\n",
       "       [-0.18952463, -1.14620805, -1.29012816, ..., -0.71357095,\n",
       "        -0.71535472, -0.00168391],\n",
       "       [-0.51110186,  0.8724218 , -0.24688138, ...,  1.40140234,\n",
       "        -0.71535472, -0.00168391],\n",
       "       [ 0.9359957 , -1.14620805,  1.4339051 , ..., -0.71357095,\n",
       "         1.39790787, -0.00168391]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('standard_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(st_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 64)                1088      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3713 (14.50 KB)\n",
      "Trainable params: 3713 (14.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## lets design the model\n",
    "## model is sequential neural network\n",
    "## Dense --> 64 [hidden layer]\n",
    "## Activation function : sigmoid, tanh, relu\n",
    "## optimizer --> Backpropagation --> updating the weights and biases --> Adam, SGD, RMSProp\n",
    "## Loss function --> Binary Cross Entropy, Categorical Cross Entropy, Sparse Categorical Cross Entropy\n",
    "## metrics --> accuracy, precision, recall, f1-score\n",
    "## Training --> logs --> folder --> tensorboard --> visualization\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "\n",
    "## Buid the ANN model\n",
    "\n",
    "## Method -1 for defining the model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy() \n",
    "\n",
    "model.compile(optimizer=opt, loss=loss, metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the tensorboard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorflow_callback = TensorBoard(log_dir = log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "11021/11021 [==============================] - 8s 719us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/500\n",
      "11021/11021 [==============================] - 8s 700us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/500\n",
      "11021/11021 [==============================] - 8s 736us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 4/500\n",
      "11021/11021 [==============================] - 8s 734us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 5/500\n",
      "11021/11021 [==============================] - 8s 720us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 6/500\n",
      "11021/11021 [==============================] - 8s 743us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 7/500\n",
      "11021/11021 [==============================] - 8s 733us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 8/500\n",
      "11021/11021 [==============================] - 8s 687us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 9/500\n",
      "11021/11021 [==============================] - 23s 2ms/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 10/500\n",
      "11021/11021 [==============================] - 8s 715us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 11/500\n",
      "11021/11021 [==============================] - 8s 719us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 12/500\n",
      "11021/11021 [==============================] - 8s 694us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 13/500\n",
      "11021/11021 [==============================] - 8s 694us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 14/500\n",
      "11021/11021 [==============================] - 8s 714us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 15/500\n",
      "11021/11021 [==============================] - 8s 717us/step - loss: nan - accuracy: 0.4330 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: nan - val_accuracy: 0.4326 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "## Setup early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=500, \n",
    "                    batch_size=32, \n",
    "                    callbacks=[tensorflow_callback, early_stopping],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/balmukundmishra/Desktop/2025-Learning/ANN_Classification/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "## save the model\n",
    "model.save('customer_churn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets load tensorboard extention and logs\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 67662), started 0:01:12 ago. (Use '!kill 67662' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-32fd0f59e2ec2909\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-32fd0f59e2ec2909\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "# Save the model\n",
    "model.save('customer_churn_model.h5')\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
    "# Save the model architecture and weights\n",
    "model_json = model.to_json()\n",
    "with open('customer_churn_model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Save the training history\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012/2012 [==============================] - 1s 286us/step\n",
      "Accuracy: 0.5263149718830584\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "Confusion Matrix:\n",
      "[[33881     0]\n",
      " [30493     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/balmukundmishra/Desktop/2025-Learning/ANN_Classification/venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "## lets work on the prediction with our .h5 file\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "# Load the model\n",
    "model = load_model('customer_churn_model.h5')\n",
    "# lets take our csv and then get prediction on that\n",
    "test_data = pd.read_csv('/Users/balmukundmishra/Desktop/2025-Learning/ANN_Classification/data/customer_churn_dataset-testing-master.csv')\n",
    "test_data.drop(columns=['CustomerID'], inplace=True)\n",
    "# let's preprocess the test data similar to training data\n",
    "# need to load my one_hot_encoder_contr.pkl\n",
    "with open('one_hot_encode_contr.pkl', 'rb') as f:\n",
    "    one_hot_encode_contr = pickle.load(f)\n",
    "with open('one_hot_encode_subs.pkl', 'rb') as f:\n",
    "    one_hot_encode_subs = pickle.load(f)\n",
    "\n",
    "with open('label_encoder.pkl', 'rb') as f:\n",
    "    L_encode = pickle.load(f)\n",
    "\n",
    "# lets encode my 'Gender' column\n",
    "test_data['Gender'] = L_encode.transform(test_data['Gender'])\n",
    "\n",
    "# lets encode my collumns Contract Length and Subscription Type\n",
    "one_hot_contract = one_hot_encode_contr.transform(test_data[['Contract Length']])\n",
    "one_hot_subsc = one_hot_encode_subs.transform(test_data[['Subscription Type']])\n",
    "# Create DataFrame from the one-hot encoded arrays\n",
    "one_hot_encoded_df = pd.DataFrame(one_hot_contract.toarray(), columns=one_hot_encode_contr.get_feature_names_out(['Contract Length']))\n",
    "one_hot_encoded_df1 = pd.DataFrame(one_hot_subsc.toarray(), columns=one_hot_encode_subs.get_feature_names_out(['Subscription Type']))\n",
    "# Concatenate the one-hot encoded DataFrames with the original DataFrame\n",
    "test_data = pd.concat([test_data, one_hot_encoded_df, one_hot_encoded_df1], axis=1)\n",
    "test_data.drop(columns=['Contract Length', 'Subscription Type'], inplace=True)\n",
    "# Preprocess the test data with the scaler\n",
    "X_test = test_data.drop(columns=['Churn'])\n",
    "#lts load my standard_scaler.pkl\n",
    "with open('standard_scaler.pkl', 'rb') as file:\n",
    "    st_scaler = pickle.load(file)\n",
    "\n",
    "X_test = st_scaler.transform(X_test)\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test)\n",
    "# Convert predictions to binary values (0 or 1)\n",
    "predictions_binary = (predictions > 0.5).astype(int)\n",
    "# Add predictions to the test data DataFrame\n",
    "test_data['Predicted Churn'] = predictions_binary\n",
    "# lets calculate the accuracy, precision, recall, f1 score and confusion matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_data['Churn'], predictions_binary)\n",
    "precision = precision_score(test_data['Churn'], predictions_binary)\n",
    "recall = recall_score(test_data['Churn'], predictions_binary)\n",
    "f1 = f1_score(test_data['Churn'], predictions_binary)\n",
    "conf_matrix = confusion_matrix(test_data['Churn'], predictions_binary)\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
